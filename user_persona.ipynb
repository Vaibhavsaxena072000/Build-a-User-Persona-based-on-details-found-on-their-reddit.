{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_d-B2oalpTy",
        "outputId": "ddea0883-37a9-4732-c177-c595b6fed26e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# This downloads the rest of TextBlob's required corpora\n",
        "!python -m textblob.download_corpora\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCNSx3yMluUS",
        "outputId": "51e84042-de7a-4759-8d88-5ebbbd75a9fd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhMprlc8kAiR",
        "outputId": "43877da1-3c54-4fb3-da34-bc4f684b0126"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "from textblob import TextBlob\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "MbFJXv_aklJ-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "from urllib.parse import urlparse\n",
        "import sys\n",
        "\n",
        "# --------- CONFIGURE THIS PART --------------\n",
        "CLIENT_ID = 'Rb_a*************j_pw'\n",
        "CLIENT_SECRET = 'TB***************CIag'\n",
        "USER_AGENT = \"windows:RedditProfileScraper:v1.0 (by /u/DevelopmentNatural62)\"\n",
        "# --------------------------------------------"
      ],
      "metadata": {
        "id": "eske2YjMfjKi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id=CLIENT_ID,\n",
        "    client_secret=CLIENT_SECRET,\n",
        "    user_agent=USER_AGENT\n",
        ")\n"
      ],
      "metadata": {
        "id": "NIn9YRBLh4DJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_username(url):\n",
        "    \"\"\"Extract the Reddit username from the profile URL.\"\"\"\n",
        "    path = urlparse(url).path.strip('/')\n",
        "    return path.split('/')[1] if path.startswith('user/') else None\n",
        "\n",
        "def fetch_user_data(username, limit=30):\n",
        "    \"\"\"Fetch recent posts and comments made by the Reddit user.\"\"\"\n",
        "    redditor = reddit.redditor(username)\n",
        "    posts, comments = [], []\n",
        "\n",
        "    for submission in redditor.submissions.new(limit=limit):\n",
        "        posts.append({\n",
        "            \"text\": submission.title + \" \" + (submission.selftext or \"\"),\n",
        "            \"subreddit\": str(submission.subreddit),\n",
        "            \"url\": submission.url\n",
        "        })\n",
        "\n",
        "    for comment in redditor.comments.new(limit=limit):\n",
        "        comments.append({\n",
        "            \"text\": comment.body,\n",
        "            \"subreddit\": str(comment.subreddit),\n",
        "            \"url\": f\"https://reddit.com{comment.permalink}\"\n",
        "        })\n",
        "\n",
        "    return posts, comments\n",
        "\n",
        "def build_user_persona(posts, comments):\n",
        "    \"\"\"Analyze content and build a persona dictionary with citations.\"\"\"\n",
        "    persona = {}\n",
        "    citations = []\n",
        "\n",
        "    all_items = posts + comments\n",
        "    all_subs = [item[\"subreddit\"] for item in all_items]\n",
        "    top_subs = Counter(all_subs).most_common(5)\n",
        "    persona[\"Top Subreddits\"] = [sub for sub, _ in top_subs]\n",
        "\n",
        "    all_text = \" \".join(item[\"text\"] for item in all_items)\n",
        "    words = [word.lower() for word in TextBlob(all_text).words if word.isalpha()]\n",
        "    keywords = Counter(words).most_common(10)\n",
        "    persona[\"Frequent Words\"] = [word for word, _ in keywords]\n",
        "\n",
        "    sentiments = [TextBlob(item[\"text\"]).sentiment.polarity for item in all_items]\n",
        "    avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0\n",
        "    if avg_sentiment > 0.1:\n",
        "        persona[\"Tone\"] = \"Positive\"\n",
        "    elif avg_sentiment < -0.1:\n",
        "        persona[\"Tone\"] = \"Negative\"\n",
        "    else:\n",
        "        persona[\"Tone\"] = \"Neutral\"\n",
        "\n",
        "    avg_len = sum(len(item[\"text\"]) for item in all_items) / len(all_items)\n",
        "    persona[\"Writing Style\"] = \"Verbose\" if avg_len > 200 else \"Concise\"\n",
        "\n",
        "    for trait in persona:\n",
        "        values = persona[trait]\n",
        "        if isinstance(values, list):\n",
        "            example = next((item for item in all_items if any(word in item[\"text\"].lower() for word in values)), None)\n",
        "        else:\n",
        "            example = all_items[0] if all_items else None\n",
        "        if example:\n",
        "            citations.append((trait, example[\"text\"][:150], example[\"url\"]))\n",
        "\n",
        "    return persona, citations\n",
        "\n",
        "def save_persona_to_txt(username, persona, citations):\n",
        "    \"\"\"Save the structured user persona with citations to a text file.\"\"\"\n",
        "    filename = f\"{username}_persona.txt\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"USER PERSONA FOR u/{username}\\n\\n\")\n",
        "        f.write(\"-----------------------------------------\\n\")\n",
        "        f.write(f\"Name: u/{username}\\n\")\n",
        "        f.write(\"Occupation: Inferred from Reddit activity\\n\")\n",
        "        f.write(\"Location: Inferred from subreddits\\n\")\n",
        "        f.write(\"Status: Inferred from comment content\\n\")\n",
        "        f.write(\"Archetype: Derived from themes (e.g. Explorer, Analyst)\\n\")\n",
        "        f.write(\"-----------------------------------------\\n\\n\")\n",
        "\n",
        "        f.write(\"MOTIVATIONS:\\n\")\n",
        "        f.write(\"- Based on common themes in posts and comments\\n\")\n",
        "        cited = next((c for c in citations if c[0] == \"Frequent Words\"), None)\n",
        "        if cited:\n",
        "            f.write(f\"📌 Cited: \\\"{cited[1]}...\\\" → {cited[2]}\\n\\n\")\n",
        "\n",
        "        f.write(\"FRUSTRATIONS:\\n\")\n",
        "        f.write(\"- Common complaints or emotional language\\n\")\n",
        "        cited = next((c for c in citations if c[0] == \"Tone\"), None)\n",
        "        if cited:\n",
        "            f.write(f\"📌 Cited: \\\"{cited[1]}...\\\" → {cited[2]}\\n\\n\")\n",
        "\n",
        "        f.write(\"BEHAVIOR & HABITS:\\n\")\n",
        "        f.write(f\"- Posts in {', '.join(persona['Top Subreddits'])}\\n\")\n",
        "        f.write(f\"- Writing style: {persona['Writing Style']}\\n\")\n",
        "        cited = next((c for c in citations if c[0] == \"Top Subreddits\"), None)\n",
        "        if cited:\n",
        "            f.write(f\"📌 Cited: \\\"{cited[1]}...\\\" → {cited[2]}\\n\\n\")\n",
        "\n",
        "        f.write(\"GOALS & NEEDS:\\n\")\n",
        "        f.write(\"- Derived from language like 'I want', 'I hope', etc.\\n\")\n",
        "        cited = next((c for c in citations if c[0] == \"Writing Style\"), None)\n",
        "        if cited:\n",
        "            f.write(f\"📌 Cited: \\\"{cited[1]}...\\\" → {cited[2]}\\n\\n\")\n",
        "\n",
        "        f.write(\"PERSONALITY:\\n\")\n",
        "        f.write(\"- Based on sentiment, writing tone and subreddit types\\n\")\n",
        "        f.write(f\"- Tone: {persona['Tone']}\\n\")\n",
        "        f.write(f\"- Style: {persona['Writing Style']}\\n\")\n",
        "        cited = next((c for c in citations if c[0] == \"Tone\"), None)\n",
        "        if cited:\n",
        "            f.write(f\"📌 Cited: \\\"{cited[1]}...\\\" → {cited[2]}\\n\\n\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "def main(profile_url):\n",
        "    \"\"\"Main function to drive Reddit scraping and persona generation.\"\"\"\n",
        "    username = extract_username(profile_url)\n",
        "    posts, comments = fetch_user_data(username)\n",
        "    persona, citations = build_user_persona(posts, comments)\n",
        "    file_path = save_persona_to_txt(username, persona, citations)\n",
        "    files.download(file_path)\n",
        "    return file_path\n",
        "\n",
        "# Example usage:\n",
        "# profile_url = \"https://www.reddit.com/user/Hungry-Move-6603/\"\n",
        "# main(profile_url)\n"
      ],
      "metadata": {
        "id": "WdjcxcCmv0MX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "profile_url = \"https://www.reddit.com/user/kojied/\"\n",
        "main(profile_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "xojtOIZEoEdT",
        "outputId": "db79d785-c2c4-482f-8884-f6e964985391"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_85619804-968c-4da1-a491-2c9d936b3f80\", \"kojied_persona.txt\", 1740)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kojied_persona.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kIxxmcjZv3cn"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}